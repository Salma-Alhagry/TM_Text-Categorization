{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP3: Statistical Language Models\n",
    "\n",
    "Hope McIntyre (hm7zg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from os import listdir\n",
    "import math\n",
    "import timeit\n",
    "import datetime\n",
    "\n",
    "# Random\n",
    "import random\n",
    "random.seed()\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer # load the stemmer module from NLTK\n",
    "stemmer = EnglishStemmer() # Get an instance of SnowballStemmer for English\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from TM3_Methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Stopword List\n",
    "stopwords = open('stopwords.txt', 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get File Names for Test and Train set\n",
    "files_test = listdir('yelp/test')\n",
    "files_train = listdir('yelp/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of rows:  38688\n"
     ]
    }
   ],
   "source": [
    "dataDF_train = load_json(files_train, 'yelp/train/')\n",
    "text_train = dataDF_train['Content']\n",
    "\n",
    "reviewID_train = dataDF_train['ReviewID']\n",
    "print(\"Num of rows: \",len(text_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of rows:  19803\n"
     ]
    }
   ],
   "source": [
    "dataDF_test = load_json(files_test, 'yelp/test/')\n",
    "text_test = dataDF_test['Content']\n",
    "\n",
    "reviewID_test = dataDF_test['ReviewID']\n",
    "print(\"Num of rows: \",len(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine Test and Train Datasets\n",
    "dataDF = dataDF_train.append(dataDF_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58491, 53653)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataDF.ReviewID), len(set(dataDF.ReviewID)) # 5000 Duplicate ReviewIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine Test and Train Content\n",
    "text = text_train.append(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize & Clean Corpus\n",
    "tokens = [clean_words(i) for i in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "tokens = [removeStopwords(i, stopwords) for i in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataDF[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Author_Location</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>Overall</th>\n",
       "      <th>ReviewID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kimberly M.</td>\n",
       "      <td>La Verne, CA</td>\n",
       "      <td>We had our very last meal in N.O here and it w...</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>5.0</td>\n",
       "      <td>oZM6XcKsYSiRoopkFDcmRA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linda R.</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>In short: this place was just alright.  It def...</td>\n",
       "      <td>2014-07-17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>TnQGrZk6g-rCgVU9V5oQ-A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stacy F.</td>\n",
       "      <td>La Mesa, CA</td>\n",
       "      <td>On a recommendation we came here for a birthda...</td>\n",
       "      <td>2014-07-16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>kdL7ImCS9meCyHjk885riQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tracy B.</td>\n",
       "      <td>Washington D.C., DC</td>\n",
       "      <td>This place was amazing! It's a little out of t...</td>\n",
       "      <td>2014-07-13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H7BH45EU_iw6d9wvNw0ZEA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Katie T.</td>\n",
       "      <td>Lisle, IL</td>\n",
       "      <td>Fresh baked rolls are out of this world. Ham h...</td>\n",
       "      <td>2014-07-15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>wi3zL1UgdbSWPbf4agTnXA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Marc H.</td>\n",
       "      <td>St. Albans, NY</td>\n",
       "      <td>This place has enough positive reviews and I'm...</td>\n",
       "      <td>2014-07-16</td>\n",
       "      <td>5.0</td>\n",
       "      <td>noDtCmuOXsswGZKhMnoZFQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Janelle R.</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Honey! My meal was great! The swimming holeThe...</td>\n",
       "      <td>2014-07-05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>RpiyxTwxusBJdNwjxTKFWw</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abby T.</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>Any of their pork dishes are delicious, and th...</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>sxGyxf0cMYDZZhhmXcyE-Q</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Susan H.</td>\n",
       "      <td>Sugar Land, TX</td>\n",
       "      <td>Three months ago, I had the distinct pleasure,...</td>\n",
       "      <td>2014-07-03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>HoF9WNRGfcleoPLFQ5dG4g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Justin L.</td>\n",
       "      <td>Memphis, TN</td>\n",
       "      <td>I was in New Orleans for business, and I was s...</td>\n",
       "      <td>2014-06-28</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AxAiSxe5Ay5pmmA5AtKkEw</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Author      Author_Location  \\\n",
       "0  Kimberly M.         La Verne, CA   \n",
       "1     Linda R.      Los Angeles, CA   \n",
       "2     Stacy F.          La Mesa, CA   \n",
       "3     Tracy B.  Washington D.C., DC   \n",
       "4     Katie T.            Lisle, IL   \n",
       "5      Marc H.       St. Albans, NY   \n",
       "6   Janelle R.    San Francisco, CA   \n",
       "7      Abby T.           Dallas, TX   \n",
       "8     Susan H.       Sugar Land, TX   \n",
       "9    Justin L.          Memphis, TN   \n",
       "\n",
       "                                             Content        Date Overall  \\\n",
       "0  We had our very last meal in N.O here and it w...  2014-07-14     5.0   \n",
       "1  In short: this place was just alright.  It def...  2014-07-17     3.0   \n",
       "2  On a recommendation we came here for a birthda...  2014-07-16     4.0   \n",
       "3  This place was amazing! It's a little out of t...  2014-07-13     5.0   \n",
       "4  Fresh baked rolls are out of this world. Ham h...  2014-07-15     5.0   \n",
       "5  This place has enough positive reviews and I'm...  2014-07-16     5.0   \n",
       "6  Honey! My meal was great! The swimming holeThe...  2014-07-05     5.0   \n",
       "7  Any of their pork dishes are delicious, and th...  2014-07-14     4.0   \n",
       "8  Three months ago, I had the distinct pleasure,...  2014-07-03     5.0   \n",
       "9  I was in New Orleans for business, and I was s...  2014-06-28     5.0   \n",
       "\n",
       "                 ReviewID  Class  \n",
       "0  oZM6XcKsYSiRoopkFDcmRA      1  \n",
       "1  TnQGrZk6g-rCgVU9V5oQ-A      0  \n",
       "2  kdL7ImCS9meCyHjk885riQ      1  \n",
       "3  H7BH45EU_iw6d9wvNw0ZEA      1  \n",
       "4  wi3zL1UgdbSWPbf4agTnXA      1  \n",
       "5  noDtCmuOXsswGZKhMnoZFQ      1  \n",
       "6  RpiyxTwxusBJdNwjxTKFWw      1  \n",
       "7  sxGyxf0cMYDZZhhmXcyE-Q      1  \n",
       "8  HoF9WNRGfcleoPLFQ5dG4g      1  \n",
       "9  AxAiSxe5Ay5pmmA5AtKkEw      1  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Binary Classification\n",
    "dataDF['Class'] = 0\n",
    "dataDF.ix[dataDF.Overall >= '4.0', 'Class'] = 1\n",
    "dataDF[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "dataDF = dataDF.reset_index()\n",
    "del dataDF['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataDF[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TF = getTermFreq(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF = getDocFreq(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list from DF with only Words with a DF of > 50\n",
    "DF_50More = [k for k, v in DF.items() if v >= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to remove all words not in wordlist\n",
    "def keepWords(text, wordList):\n",
    "    newList = [t for t in text if t in wordList]\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep Only Words with >50 DF\n",
    "tokens = [keepWords(i, DF_50More) for i in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Updated DF post Tail-Word removal\n",
    "DF = getDocFreq(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counts for Each Count Situation for Each Word\n",
    "def getWordClassProbs_fast(dataDF, textlist):\n",
    "    igWordProbs = {}\n",
    "    total1 = len(dataDF[dataDF.Class == 1])\n",
    "    total0 = len(dataDF[dataDF.Class == 0])\n",
    "    \n",
    "    for i in range(0,len(dataDF)):\n",
    "        response = dataDF['Class'].iloc[i]\n",
    "        tokenlist = set(textlist[i])\n",
    "        \n",
    "        for word in tokenlist:\n",
    "            \n",
    "            if word in igWordProbs:\n",
    "                igWordProbs[word]['P'][response] += 1\n",
    "                igWordProbs[word]['P_index'] = igWordProbs[word]['P_index']+[i]\n",
    "            else:\n",
    "                igWordProbs[word] = {}\n",
    "                igWordProbs[word]['P'] = {}\n",
    "                igWordProbs[word]['NP'] = {}\n",
    "                igWordProbs[word]['P'][0] = 0\n",
    "                igWordProbs[word]['P'][1] = 0\n",
    "                igWordProbs[word]['P'][response] = 1\n",
    "                igWordProbs[word]['P_index'] = [i]\n",
    "            \n",
    "    # Next Step is to Subset then Count and Update Dict for ['NP]\n",
    "    for word in igWordProbs.keys():\n",
    "        igWordProbs[word]['NP'][1] = total1 - igWordProbs[word]['P'][1]\n",
    "        igWordProbs[word]['NP'][0] = total0 - igWordProbs[word]['P'][0]\n",
    "    \n",
    "    return(igWordProbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Counts of Class for Each Word in Corpus\n",
    "igProbs = getWordClassProbs_fast(dataDF, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcEntropy(prob):\n",
    "    if prob != 0: \n",
    "        return prob*math.log(prob) \n",
    "    else: \n",
    "        return float(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcInfoGain(word, dataDF, igWordClassProbs): \n",
    "    # print(word)\n",
    "    obs = len(dataDF.ix[dataDF.Class])\n",
    "    prob1 = len(dataDF.ix[dataDF.Class == 1])/obs\n",
    "    prob0 = len(dataDF.ix[dataDF.Class == 0])/obs\n",
    "    # print(obs, prob1, prob0)\n",
    "\n",
    "    # If Word Present\n",
    "    obsWord_P = igWordClassProbs[word]['P'][1] + igWordClassProbs[word]['P'][0]\n",
    "\n",
    "    prob1GivenWordP = igWordClassProbs[word]['P'][1]/obsWord_P\n",
    "    prob0GivenWordP = igWordClassProbs[word]['P'][0]/obsWord_P\n",
    "\n",
    "    # If Word Not Present\n",
    "    obsWord_NP = igWordClassProbs[word]['NP'][1] + igWordClassProbs[word]['NP'][0]\n",
    "\n",
    "    prob1GivenWordNP = igWordClassProbs[word]['NP'][1]/obsWord_NP\n",
    "    prob0GivenWordNP = igWordClassProbs[word]['NP'][0]/obsWord_NP\n",
    "    \n",
    "    pWord = sum(igWordClassProbs[word]['P'].values())/obs\n",
    "    pNotWord = 1 - pWord\n",
    "\n",
    "    infoGain = - (calcEntropy(prob1) + calcEntropy(prob0)) + \\\n",
    "        pWord * (calcEntropy(prob1GivenWordP) + calcEntropy(prob0GivenWordP)) + \\\n",
    "        pNotWord * (calcEntropy(prob1GivenWordNP) + calcEntropy(prob0GivenWordNP))\n",
    "    \n",
    "    return infoGain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:55.966742\n"
     ]
    }
   ],
   "source": [
    "igAll = {word: calcInfoGain(word, dataDF, igProbs) for word in DF_50More}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 words selected by Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>noth</th>\n",
       "      <td>0.010586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bland</th>\n",
       "      <td>0.009011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delici</th>\n",
       "      <td>0.008750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediocr</th>\n",
       "      <td>0.008577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt</th>\n",
       "      <td>0.008268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect</th>\n",
       "      <td>0.007817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decent</th>\n",
       "      <td>0.007607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rude</th>\n",
       "      <td>0.007508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amaz</th>\n",
       "      <td>0.007238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mayb</th>\n",
       "      <td>0.007229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.007216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overpr</th>\n",
       "      <td>0.006209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terribl</th>\n",
       "      <td>0.006166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorit</th>\n",
       "      <td>0.006120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappoint</th>\n",
       "      <td>0.006044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0.005983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>averag</th>\n",
       "      <td>0.005921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hype</th>\n",
       "      <td>0.005466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meh</th>\n",
       "      <td>0.005257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overr</th>\n",
       "      <td>0.005203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "noth        0.010586\n",
       "bland       0.009011\n",
       "delici      0.008750\n",
       "mediocr     0.008577\n",
       "nt          0.008268\n",
       "perfect     0.007817\n",
       "decent      0.007607\n",
       "rude        0.007508\n",
       "amaz        0.007238\n",
       "mayb        0.007229\n",
       "bad         0.007216\n",
       "overpr      0.006209\n",
       "terribl     0.006166\n",
       "favorit     0.006120\n",
       "disappoint  0.006044\n",
       "worst       0.005983\n",
       "averag      0.005921\n",
       "hype        0.005466\n",
       "meh         0.005257\n",
       "overr       0.005203"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "igAllDF = pd.DataFrame.from_dict(igAll, orient = 'index')\n",
    "igAllDF.columns = ['IG']\n",
    "igAllDF = igAllDF.sort(columns = 'IG', ascending = False)\n",
    "igAllDF[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Square Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcChiSquared(word, igProbs):\n",
    "    A = igProbs[word]['P'][1]\n",
    "    B = igProbs[word]['NP'][1]\n",
    "    C = igProbs[word]['P'][0]\n",
    "    D = igProbs[word]['NP'][0]\n",
    "    try: \n",
    "        X2 = ((A+B+C+D)*((A*D)-(B*C))**2) / ((A+C)*(B+D)*(A+B)*(C+D))\n",
    "    except ZeroDivisionError:\n",
    "        X2 = float(0)\n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.155619\n"
     ]
    }
   ],
   "source": [
    "X2All = {word: calcChiSquared(word,igProbs) for word in DF_50More}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 words selected by Chi Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>noth</th>\n",
       "      <td>1455.300611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bland</th>\n",
       "      <td>1336.956776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediocr</th>\n",
       "      <td>1284.126550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rude</th>\n",
       "      <td>1122.844041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decent</th>\n",
       "      <td>1069.719398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mayb</th>\n",
       "      <td>983.070359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>966.659487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt</th>\n",
       "      <td>966.567879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overpr</th>\n",
       "      <td>924.687299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delici</th>\n",
       "      <td>916.983998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terribl</th>\n",
       "      <td>914.451957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>895.454825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>averag</th>\n",
       "      <td>850.273083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappoint</th>\n",
       "      <td>802.947016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meh</th>\n",
       "      <td>784.875986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect</th>\n",
       "      <td>780.378903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overr</th>\n",
       "      <td>780.171930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hype</th>\n",
       "      <td>773.525239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amaz</th>\n",
       "      <td>743.954653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horribl</th>\n",
       "      <td>697.054167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     X2\n",
       "noth        1455.300611\n",
       "bland       1336.956776\n",
       "mediocr     1284.126550\n",
       "rude        1122.844041\n",
       "decent      1069.719398\n",
       "mayb         983.070359\n",
       "bad          966.659487\n",
       "nt           966.567879\n",
       "overpr       924.687299\n",
       "delici       916.983998\n",
       "terribl      914.451957\n",
       "worst        895.454825\n",
       "averag       850.273083\n",
       "disappoint   802.947016\n",
       "meh          784.875986\n",
       "perfect      780.378903\n",
       "overr        780.171930\n",
       "hype         773.525239\n",
       "amaz         743.954653\n",
       "horribl      697.054167"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_DF = pd.DataFrame.from_dict(X2All, orient = 'index')\n",
    "X2_DF.columns = ['X2']\n",
    "X2_DF = X2_DF.sort(columns = 'X2', ascending = False)\n",
    "X2_DF[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove all Words/Features with X2 less then 3.841\n",
    "X2All_features = X2_DF[X2_DF.X2 > 3.841]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of your finalized controlled vocabulary after merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1870"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge InfoGain and Chi-Squared Features\n",
    "features = list(set(igAllDF.index).intersection(set(X2All_features.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1870"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter tokenized documents to only include features found from IG and X-Squared\n",
    "tokens = [keepWords(text, features) for text in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many review documents are there in the resulting corpus after feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:57.748091\n"
     ]
    }
   ],
   "source": [
    "# Create New Data set and Token List of only Documents greater then 5 words.\n",
    "dataDF_cont = dataDF\n",
    "tokens_cont = []\n",
    "\n",
    "for i in range(0,len(tokens)):\n",
    "    # print(len(tokens[i]))\n",
    "    if len(tokens[i]) < 5:\n",
    "        dataDF_cont = dataDF_cont.drop(i)\n",
    "    else:\n",
    "        tokens_cont = tokens_cont+[tokens[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56258"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_cont) # Note this number includes duplicates present in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset Index for Dataframe\n",
    "dataDF_cont = dataDF_cont.reset_index()\n",
    "del dataDF_cont['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training Naive Bayes with Maximum a Posterior estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Index Locations for 1 and 0 Classes\n",
    "indexIDs_1 = dataDF_cont.index[dataDF_cont.Class == 1]\n",
    "indexIDs_0 = dataDF_cont.index[dataDF_cont.Class == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make new list of all Reviews with Class 1 and 0 \n",
    "tokens_1 = [tokens_cont[i] for i in indexIDs_1]\n",
    "tokens_0 = [tokens_cont[i] for i in indexIDs_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Probability:  1.0000000000000016\n"
     ]
    }
   ],
   "source": [
    "# Make new list of all Reviews with Class 1\n",
    "TF_1 = getTermFreq(tokens_1)\n",
    "u_theta_1 = genUniLM(TF_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Probability:  1.000000000000006\n"
     ]
    }
   ],
   "source": [
    "# Make new list of all Reviews with Class 0\n",
    "TF_0 = getTermFreq(tokens_0)\n",
    "u_theta_0 = genUniLM(TF_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# createSmoothedProbability == Words in TF_1 and not in TF_0\n",
    "pSmooth_1 = calc_pSmoothAdditive(list(TF_0.keys()), u_theta_1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# createSmoothedProbability == Words in TF_0 and not in TF_1\n",
    "pSmooth_0 = calc_pSmoothAdditive(list(TF_1.keys()), u_theta_0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimate Naive Bayes classifier\n",
    "navBayes = {}\n",
    "for word in TF_cont:\n",
    "    logRatio = math.log(u_theta_1['p'].ix[word]/u_theta_0['p'].ix[word])\n",
    "    navBayes[word] = logRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "navBayesDF = pd.DataFrame.from_dict(navBayes, orient = \"index\")\n",
    "navBayesDF.columns = ['logRatio']\n",
    "navBayesDF = navBayesDF.sort('logRatio', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the top 20 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flawless</th>\n",
       "      <td>3.548694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radic</th>\n",
       "      <td>3.041264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>molecular</th>\n",
       "      <td>2.429462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gastronomi</th>\n",
       "      <td>2.364924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handsdown</th>\n",
       "      <td>2.313630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unforgett</th>\n",
       "      <td>2.307765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luscious</th>\n",
       "      <td>2.295931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unpretenti</th>\n",
       "      <td>2.240871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mmmmm</th>\n",
       "      <td>2.062316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scrumptious</th>\n",
       "      <td>2.002944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drawback</th>\n",
       "      <td>1.930471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marvel</th>\n",
       "      <td>1.925557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thee</th>\n",
       "      <td>1.908165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achatz</th>\n",
       "      <td>1.898248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom</th>\n",
       "      <td>1.745884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cali</th>\n",
       "      <td>1.736315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cannoli</th>\n",
       "      <td>1.704566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jesus</th>\n",
       "      <td>1.671776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heaven</th>\n",
       "      <td>1.666886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favourit</th>\n",
       "      <td>1.630954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             logRatio\n",
       "flawless     3.548694\n",
       "radic        3.041264\n",
       "molecular    2.429462\n",
       "gastronomi   2.364924\n",
       "handsdown    2.313630\n",
       "unforgett    2.307765\n",
       "luscious     2.295931\n",
       "unpretenti   2.240871\n",
       "mmmmm        2.062316\n",
       "scrumptious  2.002944\n",
       "drawback     1.930471\n",
       "marvel       1.925557\n",
       "thee         1.908165\n",
       "achatz       1.898248\n",
       "nom          1.745884\n",
       "cali         1.736315\n",
       "cannoli      1.704566\n",
       "jesus        1.671776\n",
       "heaven       1.666886\n",
       "favourit     1.630954"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "navBayesDF[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the bottom 20 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>inattent</th>\n",
       "      <td>-1.962043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horribl</th>\n",
       "      <td>-1.975672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undercook</th>\n",
       "      <td>-1.978044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meh</th>\n",
       "      <td>-1.984017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lacklust</th>\n",
       "      <td>-2.013808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrog</th>\n",
       "      <td>-2.034803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bouncer</th>\n",
       "      <td>-2.061555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>-2.062360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rude</th>\n",
       "      <td>-2.119826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediocr</th>\n",
       "      <td>-2.187307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unimpress</th>\n",
       "      <td>-2.256247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stale</th>\n",
       "      <td>-2.295229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>-2.311343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flavorless</th>\n",
       "      <td>-2.435307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overr</th>\n",
       "      <td>-2.487951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uninspir</th>\n",
       "      <td>-2.509580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unaccept</th>\n",
       "      <td>-2.690832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aok</th>\n",
       "      <td>-2.694502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tasteless</th>\n",
       "      <td>-2.770875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ined</th>\n",
       "      <td>-2.936514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            logRatio\n",
       "inattent   -1.962043\n",
       "horribl    -1.975672\n",
       "undercook  -1.978044\n",
       "meh        -1.984017\n",
       "lacklust   -2.013808\n",
       "arrog      -2.034803\n",
       "bouncer    -2.061555\n",
       "worst      -2.062360\n",
       "rude       -2.119826\n",
       "mediocr    -2.187307\n",
       "unimpress  -2.256247\n",
       "stale      -2.295229\n",
       "insult     -2.311343\n",
       "flavorless -2.435307\n",
       "overr      -2.487951\n",
       "uninspir   -2.509580\n",
       "unaccept   -2.690832\n",
       "aok        -2.694502\n",
       "tasteless  -2.770875\n",
       "ined       -2.936514"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "navBayesDF[len(navBayesDF)-20:len(navBayesDF)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Do they make sense to you in terms of distinguishing the positive opinion from negative opinion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A: Yes, these seem to make a lot of sense in terms of differentiating between positive and negative reviews. Words like heaven and flawless are in the postive group and tasteless and meh are in the negative. The terms are very polarizing and have strong conotations with either good or bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Naive Bayes as a linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56258, 0.773969924277436, 0.2260300757225639)"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = len(dataDF_cont.ix[dataDF_cont.Class])\n",
    "prob1 = len(dataDF_cont.ix[dataDF_cont.Class == 1])/obs\n",
    "prob0 = len(dataDF_cont.ix[dataDF_cont.Class == 0])/obs\n",
    "obs, prob1, prob0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcfX_NBLin(tokenlist, prob1, prob0, pSmooth_1, pSmooth_0):\n",
    "    X = 0\n",
    "    for word in tokenlist:\n",
    "        x = math.log(pSmooth_1.ix[word])-math.log(pSmooth_0.ix[word])\n",
    "        X = X + x\n",
    "    fX = math.log(prob1/prob0) + X\n",
    "\n",
    "    return fX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fX = [calcfX_NBLin(toks, prob1, prob0, pSmooth_1, pSmooth_0) for toks in tokens_cont]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort documents with respect to fX in descending order\n",
    "fXDF = pd.DataFrame({'fX': fX})\n",
    "fXDF['df_index'] = range(0,len(fXDF))\n",
    "fXDF['true'] = dataDF_cont.Class\n",
    "fXDF = fXDF.sort(columns = 'fX', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate Precision/Recall metrics while varying threshold for the P/R Plot\n",
    "mAll = []\n",
    "for i in range(0,len(fXDF)):\n",
    "    y_predicted = [1]*(i+1)+[0]*(len(fXDF)-1-i)\n",
    "    m = calcPresionRecall(fXDF.true, y_predicted)\n",
    "    mAll.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcPresionRecall(y_true, y_predicted):\n",
    "    combo = pd.DataFrame({'true': y_true,'predicted': y_predicted})\n",
    "    # nobs = len(combo)\n",
    "    TP = len(combo[(combo.true == 1) & (combo.predicted == 1)])\n",
    "    TN = len(combo[(combo.true == 0) & (combo.predicted == 0)])\n",
    "    FP = len(combo[(combo.true == 0) & (combo.predicted == 1)])\n",
    "    FN = len(combo[(combo.true == 1) & (combo.predicted == 0)])\n",
    "    \n",
    "    #TPR = truePos / (truePos + falseNeg) # Sensitivity\n",
    "    #FPR = falsePos / (trueNeg + falsePos)\n",
    "    \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    accuracy = (TP + TN) / (TP+TN+FP+FN)\n",
    "    \n",
    "    metrics = (recall, precision, accuracy)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combo = list(zip(*mAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = combo[0]\n",
    "y = combo[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXMPBblTNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115362198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the Precision-Recall Graph - Take2\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.plot(x, y, 'b')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Will we see different precision/recall tradeoffs (with alternate smoothing parameters)? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A: No, we will not see a change because the controlled vocabulary greater limited the number of unseen words. Without a significant amount of unseen words the smoothed language model will not change greatly when the delta parameter is changed. If the probabilities in the language model do not change, the Naive Bayes classification will not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: k Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56258 56258\n",
      "51678\n",
      "0:00:08.596800\n"
     ]
    }
   ],
   "source": [
    "# Build TF-IDF Vectors for all Documents\n",
    "scaled_TFIDF = createScaledTFIDFvectors(tokens_cont, dataDF_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Projection Implemention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Random Projection Vector\n",
    "def createRandomVect(vocablist):\n",
    "    vect = {}\n",
    "    for word in vocablist:\n",
    "        vect[word] = random.uniform(-1,1)\n",
    "    return vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 5 Random Projections and Store Them\n",
    "l = 5\n",
    "vectors = {i: createRandomVect(list(DF_cont.keys())) for i in range(0,l)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find Hash Bucket for Each Document\n",
    "def buildHashMap(scaled_TFIDF, randomVectors):\n",
    "    Hash = {}\n",
    "    for review in scaled_TFIDF.keys():\n",
    "        H = ''\n",
    "        for i in range(0,len(randomVectors)): # For Each Random Vector\n",
    "            dist = calc_dotProd(scaled_TFIDF[review], randomVectors[i]) # Need to Make into Dicts?\n",
    "            if dist >= 0:\n",
    "                h = 1\n",
    "            else: h = 0\n",
    "\n",
    "            H = H + str(h)\n",
    "\n",
    "        if H in Hash:\n",
    "            Hash[H] = Hash[H]+[review]\n",
    "        else: \n",
    "            Hash[H] = [review]\n",
    "    return Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getHashCode(targetDoc, randomVectors): # Make sure TargetDoc a Dictionary of Words and TF-IDF\n",
    "    H = ''\n",
    "    for i in range(0,len(randomVectors)): # For Each Random Vector\n",
    "        dist = calc_dotProd(targetDoc, randomVectors[i])\n",
    "        if dist >= 0:\n",
    "            h = 1\n",
    "        else: h = 0\n",
    "            \n",
    "        H = H + str(h)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Author_Location</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "      <th>Overall</th>\n",
       "      <th>ReviewID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cassie V.</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>Ahhh tapas, the small plates that contain indi...</td>\n",
       "      <td>2014-05-20</td>\n",
       "      <td>4.0</td>\n",
       "      <td>C0XcD26vEKq0qiXddRG0uA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>This was our last meal on our Chicago trip and...</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12345678901</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jack</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>My mouth is watering writing this review. Omg,...</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>123456789012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jack</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>In the basement of United Nations building, th...</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1234567890123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Patricia S.</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>I usually come here on friday or saturday nigh...</td>\n",
       "      <td>2014-06-18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12345678901234</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Author Author_Location  \\\n",
       "0    Cassie V.     Chicago, IL   \n",
       "1          Tom     Houston, TX   \n",
       "2         Jack     Houston, TX   \n",
       "3         Jack     Houston, TX   \n",
       "4  Patricia S.     Houston, TX   \n",
       "\n",
       "                                             Content        Date Overall  \\\n",
       "0  Ahhh tapas, the small plates that contain indi...  2014-05-20     4.0   \n",
       "1  This was our last meal on our Chicago trip and...  2015-02-01     5.0   \n",
       "2  My mouth is watering writing this review. Omg,...  2015-02-01     5.0   \n",
       "3  In the basement of United Nations building, th...  2015-02-01     5.0   \n",
       "4  I usually come here on friday or saturday nigh...  2014-06-18     4.0   \n",
       "\n",
       "                 ReviewID  Class  \n",
       "0  C0XcD26vEKq0qiXddRG0uA      1  \n",
       "1             12345678901      1  \n",
       "2            123456789012      1  \n",
       "3           1234567890123      1  \n",
       "4          12345678901234      1  "
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Validation Documents\n",
    "dataDF_val = load_json(['query.json'],'yelp/')\n",
    "text_val = dataDF_val['Content']\n",
    "# Define Binary Classification\n",
    "dataDF_val['Class'] = 0\n",
    "dataDF_val.ix[dataDF_val.Overall >= '4.0', 'Class'] = 1\n",
    "dataDF_val[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize and Reduce Validation Documents to Controlled Vocabulary\n",
    "tokens_val = [clean_words(i) for i in text_val]\n",
    "tokens_val = [keepWords(i, DF_50More) for i in tokens_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Build TF-IDF Vectors for Validation Documents\n",
    "scaled_TFIDF_val = createScaledTFIDFvectors(tokens_val, dataDF_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve K Nearest Neighbors \n",
    "def getKNearestNeighbors(targetDoc, scaled_TFIDF, randomVectors, Hash, k):\n",
    "    hashcode = getHashCode(targetDoc, randomVectors)\n",
    "    distDict = {}\n",
    "    if len(Hash[hashcode]) < k:\n",
    "        print(\"Less than 5 documents in bucket\")\n",
    "        kNN = list(Hash[hashcode].values())\n",
    "    else:\n",
    "        for review in Hash[hashcode]:\n",
    "            dist = calc_cosineDist(scaled_TFIDF[review], targetDoc)\n",
    "            distDict[review] = dist\n",
    "        distDF = pd.DataFrame.from_dict(distDict, orient = 'index')\n",
    "        distDF.columns = ['cosSim']\n",
    "        distDF = distDF.sort(columns = 'cosSim', ascending = False)\n",
    "        kNN = list(distDF.index[0:5])\n",
    "    return kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get kNN for Validation Documents\n",
    "k = 5\n",
    "kNN = {doc: getKNearestNeighbors(scaled_TFIDF_val[doc], scaled_TFIDF, vectors, Hash, k) for doc in list(dataDF_val.ReviewID)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'12345678901': ['9AfnjgED84IQ6tMoPw8fzg',\n",
       "  'YQTvnK8l3icVBJD1J2FjBA',\n",
       "  'BvI4f0m0D1bPUIy0Uzqkbw',\n",
       "  'vrB_cTO9QB5d7xsrHhbn_A',\n",
       "  'FUHe8kpJbZdieSOGkEvi6A'],\n",
       " '123456789012': ['c0vIJhNViWL6eHpeM0Dvmw',\n",
       "  '7QSUEJ4imJW9ev2o7jJ3Sg',\n",
       "  '7B1YbeAiX10Awhs3P298FA',\n",
       "  '_AcdP5RbhabCQTtVYUsWcA',\n",
       "  'SXd-Wcbbi-LR3w-dL5FvuQ'],\n",
       " '1234567890123': ['7DOxKTXeeNussrB9lYcjdg',\n",
       "  'KQc1gr9Avuxs73dU2t7Heg',\n",
       "  'EWVX3shpYMNsZS3LPj3QCQ',\n",
       "  '1m3_dTM5L8mOKQdMGpLyMg',\n",
       "  'ibUENbtGHLxApE9OUZnI6w'],\n",
       " '12345678901234': ['Vt6N0ZK8IyeuUzZR35-YRw',\n",
       "  'u8shtB2n_DTgs4v-k8ArQA',\n",
       "  'U5ewaF2kOJpWpVMQbFSnsA',\n",
       "  'YoYcYL_rMTTnc_2FTdjOaA',\n",
       "  'fX3RWMWQ8MN6W-Twli79dg'],\n",
       " 'C0XcD26vEKq0qiXddRG0uA': ['cauGQT7TruxQp-yzxyyHJg',\n",
       "  'Du7-KtWOVYzFSuiagLGySg',\n",
       "  'oQ2UtCIakHH0cB1eXzwiZA',\n",
       "  'fIdxy-6f3fIssmHSlF7uoQ',\n",
       "  '-MHSt7UnvtLNgoUb4p2Tlg']}"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the running time for brute-force k nearest neighbor search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:04.545956\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "distDict1 = {}\n",
    "targetDoc = '12345678901'\n",
    "for review in scaled_TFIDF.keys():\n",
    "    dist = calc_cosineDist(scaled_TFIDF[review], scaled_TFIDF_val[targetDoc])\n",
    "    distDict1[review] = dist\n",
    "distDF = pd.DataFrame.from_dict(distDict1, orient = 'index')\n",
    "distDF.columns = ['cosSim']\n",
    "distDF = distDF.sort(columns = 'cosSim', ascending = False)\n",
    "list(distDF.index[0:5])\n",
    "        \n",
    "runtime = datetime.datetime.now() - start\n",
    "print(str(runtime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Report the running time for approximate k=5 nearest neighbor search by random projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.515837\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "targetDoc = '12345678901'\n",
    "getKNearestNeighbors(scaled_TFIDF_val[targetDoc], scaled_TFIDF, vectors, Hash, k)\n",
    "\n",
    "runtime = datetime.datetime.now() - start\n",
    "print(str(runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Can we still find reasonable results with this approximation, i.e., does the retrieved documents help us determine the cuisine mentioned in the query document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Yes, we can still get reasonable results from this approximation. As shown by the text of the nearest neighbors found using random project similar restuarant themes can be seen within the groups as was found in MP1. For example '12345678901' was mainly about tapas/sangria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print content of kNN for each document in the validation set.\n",
    "pd.options.display.max_colwidth = 100\n",
    "for review in kNN.keys():\n",
    "    print('kNN for ReviewID: ', review)\n",
    "    for N in kNN[review]:\n",
    "        print(dataDF.Content[dataDF.ReviewID == N])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 4: Classification performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDict = {dataDF_cont.loc[i,'ReviewID']: dataDF_cont.loc[i,'Class'] for i in range(0,len(dataDF_cont))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokensDict = {dataDF_cont.loc[i,'ReviewID']: tokens_cont[i] for i in range(0,len(dataDF_cont))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 10 Test/Train Splits\n",
    "nbins = 10\n",
    "shuffledIDs = np.array(list(dataDict.keys()))\n",
    "random.shuffle(shuffledIDs)\n",
    "binKey = np.array_split(shuffledIDs, nbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getErrorType(truth,predicted):\n",
    "    if truth == 1:\n",
    "        if truth == predicted:\n",
    "            return 'TP'\n",
    "        else: return 'FN'\n",
    "    if truth == 0:\n",
    "        if truth == predicted:\n",
    "            return 'TN'\n",
    "        else: return 'FP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcPresionRecallAcc(TP,TN,FP,FN):\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    # accuracy = (TP + TN) / (TP+TN+FP+FN)\n",
    "    F1 = 2*((precision*recall)/(precision+recall))\n",
    "    \n",
    "    metrics = (recall, precision, F1)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the average precision, recall and F1 measure of Naive Bayes over these 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Naive Bayes - 10 Fold Validation\n",
    "nbins = 10\n",
    "metricsNB = []\n",
    "for i in range(0,nbins):\n",
    "    # Hold out 1 Bin\n",
    "    heldOut = i\n",
    "    bins = range(0,10)\n",
    "    trainBins = [x for x in bins if x != heldOut]\n",
    "    \n",
    "    # Get Review Ids for all Reviews in the non-heldout set\n",
    "    trainIDs = []\n",
    "    for x in trainBins:\n",
    "        trainIDs = trainIDs + list(binKey[x])\n",
    "        \n",
    "    # Make Language Models 0 and 1 from Training Set Only\n",
    "    # Make new list of all Reviews with Class 0\n",
    "    ids_0 = [k for (k,v) in dataDict.items() if v == 0 and k in trainIDs]\n",
    "    tokens_0 = [tokensDict[x] for x in ids_0]\n",
    "    TF_0 = getTermFreq(tokens_0)\n",
    "    u_theta_0 = genUniLM(TF_0)\n",
    "    \n",
    "    ids_1 = [k for (k,v) in dataDict.items() if v == 1 and k in trainIDs]\n",
    "    tokens_1 = [tokensDict[x] for x in ids_1]\n",
    "    TF_1 = getTermFreq(tokens_1)\n",
    "    u_theta_1 = genUniLM(TF_1)\n",
    "    \n",
    "    # createSmoothedProbability == Words in TF_1 and not in TF_0\n",
    "    pSmooth_0 = calc_pSmoothAdditive(list(TF_1.keys()), u_theta_0, 0.1)\n",
    "    pSmooth_1 = calc_pSmoothAdditive(list(TF_0.keys()), u_theta_1, 0.1)\n",
    "\n",
    "    # Testing\n",
    "    # Test on Held Out Set\n",
    "    \n",
    "    predicted = {}\n",
    "    true = {}\n",
    "    accCounts = {'FN':0, 'TN':0, 'TP':0,'FP':0}\n",
    "    \n",
    "    prob0 = len(ids_0)/len(tokensDict)\n",
    "    prob1 = len(ids_1)/len(tokensDict)\n",
    "    for d in binKey[heldOut]:\n",
    "        fX = calcfX_NBLin(tokensDict[d], prob1, prob0, pSmooth_1, pSmooth_0)\n",
    "        if fX >= 0:\n",
    "            predicted = 1\n",
    "        else: predicted = 0\n",
    "\n",
    "        truth = dataDict[d] # true class value in dataDict\n",
    "        accCounts[getErrorType(truth,predicted)] +=1\n",
    "    \n",
    "    print(accCounts['TP'],accCounts['TN'],accCounts['FP'],accCounts['FN'])\n",
    "    metrics = calcPresionRecallAcc(accCounts['TP'],accCounts['TN'],accCounts['FP'],accCounts['FN'])\n",
    " \n",
    "    metricsNB = metricsNB + [metrics] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For NB -  precision:  0.903902738916 recall:  0.905403494481 F1:  0.904645875054\n"
     ]
    }
   ],
   "source": [
    "x = [np.mean(x) for x in zip(*metricsNB)]\n",
    "print(\"For NB - \",'precision: ',x[0], 'recall: ',x[1],'F1: ',x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the average precision, recall and F1 measure of kNN over these 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kNN\n",
    "nbins = 10\n",
    "randomVectors = vectors\n",
    "metricskNN = []\n",
    "for i in range(0,nbins):\n",
    "    # Hold out 1 Bin\n",
    "    heldOut = i\n",
    "    bins = range(0,10)\n",
    "    trainBins = [x for x in bins if x != heldOut]\n",
    "    \n",
    "    # Get Review Ids for all Reviews in the non-heldout set\n",
    "    trainIDs = []\n",
    "    for x in trainBins:\n",
    "        trainIDs = trainIDs + list(binKey[x])\n",
    "    \n",
    "    # Filter TF-IDF dictionary to only in trainlist\n",
    "    trainTFIDF = {ID: scaled_TFIDF[ID] for ID in trainIDs}\n",
    "    trainHash = buildHashMap(trainTFIDF, randomVectors)\n",
    "\n",
    "    # Testing\n",
    "    predicted = {}\n",
    "    true = {}\n",
    "    accCounts = {'FN':0, 'TN':0, 'TP':0,'FP':0}\n",
    "    for d in binKey[heldOut]:\n",
    "        # print(targetdoc)\n",
    "        kNN = getKNearestNeighbors(scaled_TFIDF[d], trainTFIDF, randomVectors, trainHash, k=5)\n",
    "        \n",
    "        # Majority Vote from Neighbors on Class\n",
    "        pred = []\n",
    "        for n in kNN:\n",
    "            Class = dataDict[n]\n",
    "            pred = pred + [Class]\n",
    "        #predicted[targetDoc] = most_common(pred)\n",
    "        #true[targetDoc] = dataDict_val[targetDoc] # true class value in dataDict\n",
    "        \n",
    "        predicted = most_common(pred)\n",
    "        truth = dataDict[d] # true class value in dataDict\n",
    "        accCounts[getErrorType(truth,predicted)] +=1\n",
    "    \n",
    "    print(accCounts['TP'],accCounts['TN'],accCounts['FP'],accCounts['FN'])\n",
    "    metrics = calcPresionRecallAcc(accCounts['TP'],accCounts['TN'],accCounts['FP'],accCounts['FN'])\n",
    "    metricskNN = metricskNN + [metrics] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kNN -  precision:  0.92526114765 recall:  0.801157984603 F1:  0.85873010189\n"
     ]
    }
   ],
   "source": [
    "x = [np.mean(x) for x in zip(*metricskNN)]\n",
    "print(\"For kNN - \",'precision: ',x[0], 'recall: ',x[1],'F1: ',x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform paired two sample t-test to verify if one classifier is significantly better than another (set the confidence level to be 95%)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform2sampleTtest(Y1, Y2, N1, N2, s1, s2, alpha):\n",
    "    T = (Y1-Y2)/math.sqrt((s1**2/N1)+(s2**2/N2)) \n",
    "\n",
    "    # df = (N1 + N2) / 2 # if variances are assumed to be equal\n",
    "    df = (((s1**2/N1)+(s2**2/N2))**2) / (((s1**2/N1)**2/(N1-1))+((s2**2/N2)**2/(N2-1)))\n",
    "\n",
    "    # Assume two-tail test, t-statistic value\n",
    "\n",
    "    t = stats.t.ppf(1-alpha/2, df)\n",
    "    if abs(T) > t:\n",
    "        print('Reject the null hypothesis that the two classifiers are equal')\n",
    "    else:\n",
    "        print('Do not reject the null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reject the null hypothesis that the two classifiers are equal\n"
     ]
    }
   ],
   "source": [
    "# paired two sample t-test to verify if one classifier by F1 Score is significantly better than another \n",
    "\n",
    "from scipy import stats\n",
    "alpha = 0.05 # 95%\n",
    "N1 = 10 # Number of Folds\n",
    "N2 = 10 # Number of Folds\n",
    "s1 = [np.var(x) for x in zip(*metricsNB)][2]\n",
    "s2 = [np.var(x) for x in zip(*metricskNN)][2]\n",
    "Y1 = [np.mean(x) for x in zip(*metricsNB)][2]\n",
    "Y2 = [np.mean(x) for x in zip(*metricskNN)][2]\n",
    "\n",
    "# Perform t-test on Precision\n",
    "perform2sampleTtest(Y1, Y2, N1, N2, s1, s2, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that one classifier is significantly better than the other according to the F1 score at 95% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  If so, which one is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the Naive Bayes performed better because it had a higher average F1-Score meaning a better average performance between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Explore the best configuration of l and k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To explore the best configuration of l and k I will use a grid search for the hyperparameter optimization. The grid will consist of all combinations of l and k between 3,6. The combinations are as follows: [(3, 3), (3, 4), (3, 5), (3, 6), (4, 3), (4, 4), (4, 5), (4, 6), (5, 3), (5, 4), (5, 5), (5, 6), (6, 3), (6, 4), (6, 5), (6, 6)].\n",
    "Performance will be evaluated by F1 Score/runTime. Since 'l' will continue to improve F1 as the l decreases, but the computation time will increase dramactically the combined metric will allow for a quantiative evaluation of the trade-off. A smaller data set (test documents only) was used for the hyperparameter optimization to reduce runtime per cycle. It is assumed that this test set is representative of the whole sample and sufficiently large enough to evaluate the gain from different l,k parameters. Additionally, the hyperparameter optimization was completed on 4-fold validation set to again reduce runtime of each cycle, but maintain a representative evaluation of test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build search space, All combinations of 2 between 3 and 7.\n",
    "import itertools\n",
    "searchSpace = list(itertools.product(list(range(3,7)),list(range(3,7))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Test/Train Splits\n",
    "nbins = 4\n",
    "shuffledIDs = np.array(list(dataDict.keys()))\n",
    "random.shuffle(shuffledIDs)\n",
    "binKey = np.array_split(shuffledIDs, nbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runkNNwithKNNandLVectors(nbins, l, k, dataDict = dataDict, DF_cont = DF_cont, scaled_TFIDF = scaled_TFIDF):    \n",
    "    # Create l Random Projections and Store Them\n",
    "    randomVectors = {i: createRandomVect(list(DF_cont.keys())) for i in range(0,l)}\n",
    "    metricskNN = []\n",
    "    for i in range(0,nbins):\n",
    "        # Hold out 1 Bin\n",
    "        heldOut = i\n",
    "        bins = range(0,nbins)\n",
    "        trainBins = [x for x in bins if x != heldOut]\n",
    "\n",
    "        # Get Review Ids for all Reviews in the non-heldout set\n",
    "        trainIDs = []\n",
    "        for x in trainBins:\n",
    "            trainIDs = trainIDs + list(binKey[x])\n",
    "\n",
    "        # Filter TF-IDF dictionary to only in trainlist\n",
    "        trainTFIDF = {ID: scaled_TFIDF[ID] for ID in trainIDs}\n",
    "        trainHash = buildHashMap(trainTFIDF, randomVectors)\n",
    "\n",
    "        # Testing\n",
    "        predicted = {}\n",
    "        true = {}\n",
    "        accCounts = {'FN':0, 'TN':0, 'TP':0,'FP':0}\n",
    "        for d in binKey[heldOut]:\n",
    "            # print(targetdoc)\n",
    "            kNN = getKNearestNeighbors(scaled_TFIDF[d], trainTFIDF, randomVectors, trainHash, k)\n",
    "\n",
    "            # Majority Vote from Neighbors on Class\n",
    "            pred = []\n",
    "            for n in kNN:\n",
    "                Class = dataDict[n]\n",
    "                pred = pred + [Class]\n",
    "            #predicted[targetDoc] = most_common(pred)\n",
    "            #true[targetDoc] = dataDict_val[targetDoc] # true class value in dataDict\n",
    "\n",
    "            predicted = most_common(pred)\n",
    "            truth = dataDict[d] # true class value in dataDict\n",
    "            accCounts[getErrorType(truth,predicted)] +=1\n",
    "\n",
    "        print(accCounts['TP'],accCounts['TN'],accCounts['FP'],accCounts['FN'])\n",
    "        metrics = calcPresionRecallAcc(accCounts['TP'],accCounts['TN'],accCounts['FP'],accCounts['FN'])\n",
    "        metricskNN = metricskNN + [metrics]\n",
    "    avgF1 = [np.mean(x) for x in zip(*metricskNN)][2]\n",
    "    return avgF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tuple (l,k) in searchSpace set run kNN and store F1 metric in a dictionary\n",
    "\n",
    "results = {}\n",
    "for t in searchSpace:\n",
    "    print(t)\n",
    "    start = datetime.datetime.now()\n",
    "    avgF1 = runkNNwithKNNandLVectors(nbins = 4, l=t[0], k=t[1])\n",
    "    results[t] = avgF1\n",
    "    runTime = datetime.datetime.now() - start\n",
    "    print(str(runTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Results of F1 from parameter optimization across search space, performed in another kernel (l, k)\n",
    "resultsImport = {(3, 3): 0.88264620035319652,\n",
    " (3, 4): 0.8814689975823472,\n",
    " (3, 5): 0.8810198300108476,\n",
    " (3, 6): 0.88226383140147102,\n",
    " (4, 3): 0.87781495071839455,\n",
    " (4, 4): 0.8790340348342226,\n",
    " (4, 5): 0.87990170518039479,\n",
    " (4, 6): 0.87891960055827367,\n",
    " (5, 3): 0.87707545799950171,\n",
    " (5, 4): 0.87685527336127533,\n",
    " (5, 5): 0.87156628390704527,\n",
    " (5, 6): 0.87421291008395519,\n",
    " (6, 3): 0.87472004232895384,\n",
    " (6, 4): 0.87205083473488654,\n",
    " (6, 5): 0.87239442261958666,\n",
    " (6, 6): 0.87228927197866557}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 3)]"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finds optimal l,k combination within search space by F1 Score only\n",
    "max_F1 = max(resultsImport.values())             #finds the max value\n",
    "max_keys = [k for k,v in resultsImport.items() if v == max_F1]\n",
    "max_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find runtime, runtimes from one cycle of each value of l with k constant (k=5) was used as an estimate for runtime with l at that parameter value. Values of runtime were round to the nearest minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3, 5) - 0:14:51.478454\n",
    "(4, 5) - 0:07:22.174533\n",
    "(5, 5) - 0:04:02.294308\n",
    "(6, 5) - 0:02:39.817204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runtimeImport = {3: 15,\n",
    "4: 7,\n",
    "5: 4,\n",
    "6: 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finds optimal l,k combination within search space by F1 Score / runTime\n",
    "finalScore = {}\n",
    "for t in resultsImport.keys():\n",
    "    print(t)\n",
    "    finalScore[t] = resultsImport[t]/runtimeImport[t[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 3)]"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_score = max(finalScore.values())             #finds the max value\n",
    "max_score_keys = [k for k,v in finalScore.items() if v == max_score]\n",
    "max_score_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the metric factoring in runtime, the optimal l,k parameter within my searchspace would be l=6 and k=3. Given this result I would likely extend my searchspace to continue evaluating several additional larger l values (i.e. l=7,8). But given this search and experiment design (l,k) of (6,3) would be my choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
