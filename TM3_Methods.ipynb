{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from os import listdir\n",
    "import math\n",
    "import timeit\n",
    "import datetime\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer # load the stemmer module from NLTK\n",
    "stemmer = EnglishStemmer() # Get an instance of SnowballStemmer for English\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_json(fileList,filePath):\n",
    "    dataDF = pd.DataFrame()\n",
    "    for file in fileList:\n",
    "        #json_data = open(filePath+file, encoding = \"ISO-8859-1\").read()\n",
    "        json_data = open(filePath+file, errors = \"ignore\").read()\n",
    "        data = json.loads(json_data)\n",
    "\n",
    "        # Move Reviews only to DataFrame\n",
    "        dataDF = dataDF.append(pd.DataFrame.from_dict(data['Reviews']))\n",
    "        \n",
    "    return dataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_words(text):\n",
    "    tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "    words = tokenizer.tokenize(text)\n",
    "\n",
    "    # Convert to Lowercase\n",
    "    # words = words.map(str.lower)aa\n",
    "    cleanWords = [t.lower() for t in words]\n",
    "\n",
    "    # Normalize (remove punctuation)\n",
    "    # punc = string.punctuation\n",
    "    # cleanWords = [t for t in cleanWords if t not in punc]\n",
    "    cleanWords = [re.sub('[^0-9a-z]', \"\", x) for x in cleanWords]\n",
    "    \n",
    "    # Remove Empty Vectors\n",
    "    cleanWords = [x for x in cleanWords if x != '']\n",
    " \n",
    "    # Identify Digits & Convert to Num\n",
    "    cleanWords = [re.sub(\"\\d+\", \"NUM\", x) for x in cleanWords]\n",
    "\n",
    "    # Stem Words\n",
    "    cleanWords = [stemmer.stem(x) for x in cleanWords] # call stemmer to stem the input\n",
    "    \n",
    "    return cleanWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words (From 1-grams)\n",
    "def removeStopwords(text, stopwordList):\n",
    "    newList = [t for t in text if t not in stopwordList]\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTermFreq(textList):\n",
    "    TF = {}\n",
    "    for row in textList:\n",
    "        #print(row)\n",
    "        for word in row:\n",
    "            # print(word)\n",
    "            if word in TF:\n",
    "                TF[word] += 1\n",
    "            else:\n",
    "                TF[word] = 1\n",
    "    return TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDocFreq(textlist):\n",
    "    DF = {}\n",
    "    for row in textlist:\n",
    "        for word in set(row):\n",
    "            # print(word)\n",
    "            if word in DF:\n",
    "                DF[word] += 1\n",
    "            else:\n",
    "                DF[word] = 1\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unigram Language Model\n",
    "def genUniLM(TF):\n",
    "    u_theta = pd.DataFrame.from_dict(TF, orient = \"index\")\n",
    "    u_theta.columns = ['TF']\n",
    "    # u_theta.sort('TF', ascending = False)[0:10]\n",
    "    # Total Number of Words in Training Corpus\n",
    "    nWords = u_theta['TF'].sum()\n",
    "    nWords\n",
    "    # Number of Unique Words in Training Corpus\n",
    "    vSize = len(u_theta['TF'])\n",
    "    vSize\n",
    "    # Calculate Probabilty of Each Word by TTF/N\n",
    "    u_theta['p'] = u_theta/nWords\n",
    "    u_theta = u_theta.sort('TF', ascending = False)\n",
    "    # Check that Probability Sums to 1\n",
    "    print(\"Total Probability: \",u_theta['p'].sum())\n",
    "    return u_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_pSmoothAdditive(tokenList, u_theta, d):\n",
    "    \n",
    "    vSize_train = len(u_theta)\n",
    "    nWords_train = sum(u_theta['TF'])\n",
    "    \n",
    "    unseenWords = list(set(tokenList) - set(u_theta.index))\n",
    "    #print(len(unseenWords))\n",
    "    if len(unseenWords) == 0:\n",
    "        return u_theta['p']\n",
    "    else:\n",
    "        # Build Series with all unique words in training set + unseen words from test document\n",
    "        pSmooth = u_theta['TF'].append(pd.Series(([0]*len(unseenWords)), index = unseenWords))\n",
    "        nWords_train += len(unseenWords)\n",
    "        vSize_train += len(unseenWords)\n",
    "        f = lambda x: ((x + d) / (nWords_train + d*vSize_train))\n",
    "        pSmooth = pSmooth.map(f)\n",
    "        return pSmooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_countVectors(tokens):\n",
    "    doc_TF = {}\n",
    "    for token in tokens:\n",
    "        if token in doc_TF:\n",
    "            doc_TF[token] += 1\n",
    "        else:\n",
    "            doc_TF[token] = 1\n",
    "    return doc_TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createScaledTFIDFvectors(textlist, dataDF):\n",
    "    \n",
    "    # Make Dictionary of Dictionaries for textlist\n",
    "    doc_TF = {}\n",
    "    for i in range(0,len(dataDF)):\n",
    "        doc_TF[dataDF.ix[i,'ReviewID']] = create_countVectors(textlist[i])\n",
    "    # print(len(doc_TF.keys()))\n",
    "    # Calculate TF with Sub-linear TF scaling\n",
    "    TF_scaled = {}\n",
    "    for review in doc_TF.keys():\n",
    "        TF_scaled[review] = {}\n",
    "        for key in doc_TF[review].keys():\n",
    "            TF_scaled[review][key] = (1 + math.log(doc_TF[review][key]))\n",
    "\n",
    "    DF = getDocFreq(textlist)\n",
    "\n",
    "    scaled_TFIDF = {}\n",
    "    n_doc = len(doc_TF.keys())\n",
    "    for review in doc_TF.keys():\n",
    "        scaled_TFIDF[review] = {}\n",
    "        for key in doc_TF[review].keys():\n",
    "            if key in DF.keys():\n",
    "                scaled_TFIDF[review][key] = TF_scaled[review][key] * (1 + math.log((n_doc/DF[key])))\n",
    "            else:\n",
    "                scaled_TFIDF[review][key] = TF_scaled[review][key]\n",
    "        \n",
    "    return scaled_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_norm(vecDict):\n",
    "    norm = 0\n",
    "    for value in vecDict.values():\n",
    "        #print(key, value)\n",
    "        norm = norm + value**2\n",
    "        #print(tot)\n",
    "    norm = math.sqrt(norm)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dotProd(vecDict1,vecDict2):\n",
    "    totDot = 0\n",
    "    comWords = set(vecDict1.keys() & vecDict2.keys())\n",
    "    for word in comWords:\n",
    "        vec1_count = vecDict1[word]\n",
    "        vec2_count = vecDict2[word]\n",
    "        totDot = totDot + (vec1_count * vec2_count)\n",
    "    return totDot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cosineDist(vecDict1,vecDict2):\n",
    "    dist = calc_dotProd(vecDict1,vecDict2)/(calc_norm(vecDict1) * calc_norm(vecDict2))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
